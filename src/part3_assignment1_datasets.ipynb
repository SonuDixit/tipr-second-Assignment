{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helper_fns_from_1 import data_read_convert_to_np_array, preprocess_text_data, read_label_from_text_file\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from NN import NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dolphin dataset, MLNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolph_data = data_read_convert_to_np_array(data_path = \"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\dolphins\\\\dolphins.csv\")\n",
    "dolph_label = data_read_convert_to_np_array(data_path = \"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\dolphins\\\\dolphins_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dolph_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(dolph_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dolph_data, dolph_label, test_size=0.20, random_state=42,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_one_hot = np.zeros((y_train.shape[0],4))\n",
    "y_test_one_hot = np.zeros((y_test.shape[0],4))\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_tr_one_hot[i,y_train[i]] = 1\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_one_hot[i,y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.Sequential([\n",
    "    keras.layers.Dense(20, input_shape=(32,),activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(4, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43 samples, validate on 19 samples\n",
      "Epoch 1/19\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 1.4381 - acc: 0.2326 - val_loss: 1.3873 - val_acc: 0.3158\n",
      "Epoch 2/19\n",
      "43/43 [==============================] - 0s 363us/step - loss: 1.4256 - acc: 0.2558 - val_loss: 1.3783 - val_acc: 0.3158\n",
      "Epoch 3/19\n",
      "43/43 [==============================] - 0s 456us/step - loss: 1.4148 - acc: 0.2558 - val_loss: 1.3701 - val_acc: 0.3158\n",
      "Epoch 4/19\n",
      "43/43 [==============================] - 0s 279us/step - loss: 1.4046 - acc: 0.2558 - val_loss: 1.3627 - val_acc: 0.3158\n",
      "Epoch 5/19\n",
      "43/43 [==============================] - 0s 279us/step - loss: 1.3958 - acc: 0.2558 - val_loss: 1.3560 - val_acc: 0.3684\n",
      "Epoch 6/19\n",
      "43/43 [==============================] - 0s 256us/step - loss: 1.3870 - acc: 0.2791 - val_loss: 1.3492 - val_acc: 0.3684\n",
      "Epoch 7/19\n",
      "43/43 [==============================] - 0s 233us/step - loss: 1.3787 - acc: 0.2791 - val_loss: 1.3429 - val_acc: 0.3684\n",
      "Epoch 8/19\n",
      "43/43 [==============================] - 0s 233us/step - loss: 1.3702 - acc: 0.3023 - val_loss: 1.3369 - val_acc: 0.3684\n",
      "Epoch 9/19\n",
      "43/43 [==============================] - 0s 279us/step - loss: 1.3632 - acc: 0.3023 - val_loss: 1.3320 - val_acc: 0.3684\n",
      "Epoch 10/19\n",
      "43/43 [==============================] - 0s 256us/step - loss: 1.3566 - acc: 0.3256 - val_loss: 1.3274 - val_acc: 0.3684\n",
      "Epoch 11/19\n",
      "43/43 [==============================] - 0s 233us/step - loss: 1.3498 - acc: 0.3953 - val_loss: 1.3221 - val_acc: 0.4211\n",
      "Epoch 12/19\n",
      "43/43 [==============================] - 0s 256us/step - loss: 1.3435 - acc: 0.4419 - val_loss: 1.3170 - val_acc: 0.4211\n",
      "Epoch 13/19\n",
      "43/43 [==============================] - 0s 256us/step - loss: 1.3373 - acc: 0.4186 - val_loss: 1.3119 - val_acc: 0.4211\n",
      "Epoch 14/19\n",
      "43/43 [==============================] - 0s 233us/step - loss: 1.3312 - acc: 0.4419 - val_loss: 1.3073 - val_acc: 0.4737\n",
      "Epoch 15/19\n",
      "43/43 [==============================] - 0s 279us/step - loss: 1.3257 - acc: 0.4419 - val_loss: 1.3028 - val_acc: 0.4737\n",
      "Epoch 16/19\n",
      "43/43 [==============================] - 0s 256us/step - loss: 1.3201 - acc: 0.4419 - val_loss: 1.2982 - val_acc: 0.4211\n",
      "Epoch 17/19\n",
      "43/43 [==============================] - 0s 279us/step - loss: 1.3143 - acc: 0.4651 - val_loss: 1.2935 - val_acc: 0.4211\n",
      "Epoch 18/19\n",
      "43/43 [==============================] - 0s 279us/step - loss: 1.3087 - acc: 0.4651 - val_loss: 1.2890 - val_acc: 0.4737\n",
      "Epoch 19/19\n",
      "43/43 [==============================] - 0s 256us/step - loss: 1.3035 - acc: 0.5116 - val_loss: 1.2845 - val_acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3e5ec898>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(dolph_data,dolph_label,epochs=19,validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dolph = NN(hidden_layer=[30,4], \n",
    "                activation=[\"tanh\",\"relu\"], \n",
    "                input_dim=32,output_dim=4,momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 loss =1.2733400474010232\n",
      "epoch = 2 loss =0.9834740615602726\n",
      "epoch = 3 loss =0.7764339577684372\n",
      "epoch = 4 loss =0.7190874529430265\n",
      "epoch = 5 loss =0.6828130177420096\n",
      "epoch = 6 loss =0.6550770256681598\n",
      "epoch = 7 loss =0.6139541926149509\n",
      "epoch = 8 loss =0.5387398214174133\n",
      "epoch = 9 loss =0.4247510288675874\n",
      "epoch = 10 loss =0.3559162716691545\n",
      "epoch = 11 loss =0.34251144907647585\n",
      "epoch = 12 loss =0.30578019245826377\n",
      "epoch = 13 loss =0.30459097644509386\n",
      "epoch = 14 loss =0.268126036812998\n",
      "epoch = 15 loss =0.319825866994034\n",
      "epoch = 16 loss =0.28670025812064764\n",
      "epoch = 17 loss =0.23010552869704096\n",
      "epoch = 18 loss =0.24635910048538306\n",
      "epoch = 19 loss =0.17590524670771437\n",
      "epoch = 20 loss =0.1928258258098528\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "loss = net_dolph.fit_batch(X_train,y_tr_one_hot,epochs=20,lr=0.1,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals = net_dolph.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,pred_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pubmed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_data = data_read_convert_to_np_array(data_path = \"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\pubmed\\\\pubmed.csv\")\n",
    "pubmed_label = data_read_convert_to_np_array(data_path = \"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\pubmed\\\\pubmed_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20701, 128)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(pubmed_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pubmed_data, pubmed_label, test_size=0.40, random_state=42,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_one_hot = np.zeros((y_train.shape[0],3))\n",
    "y_test_one_hot = np.zeros((y_test.shape[0],3))\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_tr_one_hot[i,y_train[i]] = 1\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_one_hot[i,y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pubmed = NN(hidden_layer=[30,30,10], \n",
    "                activation=[\"tanh\",\"relu\",\"relu\"], \n",
    "                input_dim=128,output_dim=3,\n",
    "                momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12420, 128)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 loss =1.066639516958375\n",
      "epoch = 2 loss =1.0635175953671563\n",
      "epoch = 3 loss =1.0591347368963195\n",
      "epoch = 4 loss =1.0580674434604835\n",
      "epoch = 5 loss =1.0574425107044816\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "loss = net_pubmed.fit_batch(X_train,y_tr_one_hot,epochs=5,lr=0.001,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 loss =1.0793365177544896\n",
      "epoch = 2 loss =1.07920482291227\n",
      "epoch = 3 loss =1.07920482291227\n",
      "epoch = 4 loss =1.0792048229122697\n",
      "epoch = 5 loss =1.0792048229122697\n",
      "epoch = 6 loss =1.07920482291227\n",
      "epoch = 7 loss =1.07920482291227\n",
      "epoch = 8 loss =1.0792048229122697\n",
      "epoch = 9 loss =1.0792048229122697\n",
      "epoch = 10 loss =1.07920482291227\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "loss = net_pubmed.fit_batch(X_train,y_tr_one_hot,epochs=10,lr=0.01,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39330998671658013"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vals = net_pubmed.predict(X_test)\n",
    "accuracy_score(y_test,pred_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12420 samples, validate on 8281 samples\n",
      "Epoch 1/10\n",
      "12420/12420 [==============================] - 3s 203us/step - loss: 1.0616 - acc: 0.4306 - val_loss: 1.0642 - val_acc: 0.4254\n",
      "Epoch 2/10\n",
      "12420/12420 [==============================] - 2s 149us/step - loss: 1.0473 - acc: 0.4491 - val_loss: 1.0653 - val_acc: 0.4282\n",
      "Epoch 3/10\n",
      "12420/12420 [==============================] - 2s 149us/step - loss: 1.0418 - acc: 0.4548 - val_loss: 1.0673 - val_acc: 0.4264\n",
      "Epoch 4/10\n",
      "12420/12420 [==============================] - 2s 149us/step - loss: 1.0362 - acc: 0.4622 - val_loss: 1.0718 - val_acc: 0.4233\n",
      "Epoch 5/10\n",
      "12420/12420 [==============================] - 2s 149us/step - loss: 1.0317 - acc: 0.4663 - val_loss: 1.0747 - val_acc: 0.4221\n",
      "Epoch 6/10\n",
      "12420/12420 [==============================] - 2s 150us/step - loss: 1.0255 - acc: 0.4719 - val_loss: 1.0777 - val_acc: 0.4183\n",
      "Epoch 7/10\n",
      "12420/12420 [==============================] - 2s 149us/step - loss: 1.0208 - acc: 0.4775 - val_loss: 1.0783 - val_acc: 0.4158\n",
      "Epoch 8/10\n",
      "12420/12420 [==============================] - 2s 149us/step - loss: 1.0154 - acc: 0.4804 - val_loss: 1.0846 - val_acc: 0.4124\n",
      "Epoch 9/10\n",
      "12420/12420 [==============================] - 2s 149us/step - loss: 1.0103 - acc: 0.4877 - val_loss: 1.0904 - val_acc: 0.4062\n",
      "Epoch 10/10\n",
      "12420/12420 [==============================] - 2s 150us/step - loss: 1.0047 - acc: 0.4940 - val_loss: 1.0963 - val_acc: 0.4067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x49afa978>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = keras.Sequential([\n",
    "    keras.layers.Dense(30, input_shape=(128,),activation=tf.nn.tanh),\n",
    "    keras.layers.Dense(30, activation=tf.nn.relu),\n",
    "#     keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "])\n",
    "model2.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model2.fit(pubmed_data,pubmed_label,epochs=10,validation_split = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12420 samples, validate on 8281 samples\n",
      "Epoch 1/10\n",
      "12420/12420 [==============================] - 2s 146us/step - loss: 1.0077 - acc: 0.4891 - val_loss: 1.0967 - val_acc: 0.4107\n",
      "Epoch 2/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 1.0035 - acc: 0.4977 - val_loss: 1.0956 - val_acc: 0.4090\n",
      "Epoch 3/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 0.9993 - acc: 0.4980 - val_loss: 1.1036 - val_acc: 0.4056\n",
      "Epoch 4/10\n",
      "12420/12420 [==============================] - 2s 148us/step - loss: 0.9947 - acc: 0.5035 - val_loss: 1.1039 - val_acc: 0.4006\n",
      "Epoch 5/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 0.9896 - acc: 0.5054 - val_loss: 1.1139 - val_acc: 0.4014\n",
      "Epoch 6/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 0.9875 - acc: 0.5137 - val_loss: 1.1158 - val_acc: 0.4031\n",
      "Epoch 7/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 0.9821 - acc: 0.5129 - val_loss: 1.1220 - val_acc: 0.3998\n",
      "Epoch 8/10\n",
      "12420/12420 [==============================] - 2s 145us/step - loss: 0.9778 - acc: 0.5217 - val_loss: 1.1211 - val_acc: 0.3990\n",
      "Epoch 9/10\n",
      "12420/12420 [==============================] - 2s 146us/step - loss: 0.9738 - acc: 0.5201 - val_loss: 1.1381 - val_acc: 0.3905\n",
      "Epoch 10/10\n",
      "12420/12420 [==============================] - 2s 146us/step - loss: 0.9707 - acc: 0.5221 - val_loss: 1.1390 - val_acc: 0.3951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3ff1c4e0>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(pubmed_data,pubmed_label,epochs=10,validation_split = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_data = preprocess_text_data(\"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\twitter\\\\twitter.txt\")\n",
    "twit_label = read_label_from_text_file(\"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\twitter\\\\twitter_label.txt\")\n",
    "twit_label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "t = pca.fit_transform(twit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(twit_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 2845)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/10\n",
      "4200/4200 [==============================] - 1s 355us/step - loss: 0.9695 - acc: 0.5010 - val_loss: 0.9425 - val_acc: 0.5450\n",
      "Epoch 2/10\n",
      "4200/4200 [==============================] - 1s 156us/step - loss: 0.8649 - acc: 0.5876 - val_loss: 0.9335 - val_acc: 0.5500\n",
      "Epoch 3/10\n",
      "4200/4200 [==============================] - 1s 155us/step - loss: 0.8443 - acc: 0.5998 - val_loss: 0.9388 - val_acc: 0.5411\n",
      "Epoch 4/10\n",
      "4200/4200 [==============================] - 1s 158us/step - loss: 0.8313 - acc: 0.6040 - val_loss: 0.9380 - val_acc: 0.5433\n",
      "Epoch 5/10\n",
      "4200/4200 [==============================] - 1s 154us/step - loss: 0.8188 - acc: 0.6121 - val_loss: 0.9361 - val_acc: 0.5356\n",
      "Epoch 6/10\n",
      "4200/4200 [==============================] - 1s 155us/step - loss: 0.8079 - acc: 0.6183 - val_loss: 0.9387 - val_acc: 0.5361\n",
      "Epoch 7/10\n",
      "4200/4200 [==============================] - 1s 159us/step - loss: 0.7949 - acc: 0.6388 - val_loss: 0.9385 - val_acc: 0.5506\n",
      "Epoch 8/10\n",
      "4200/4200 [==============================] - 1s 154us/step - loss: 0.7838 - acc: 0.6390 - val_loss: 0.9429 - val_acc: 0.5456\n",
      "Epoch 9/10\n",
      "4200/4200 [==============================] - 1s 154us/step - loss: 0.7693 - acc: 0.6507 - val_loss: 0.9557 - val_acc: 0.5394\n",
      "Epoch 10/10\n",
      "4200/4200 [==============================] - 1s 156us/step - loss: 0.7548 - acc: 0.6671 - val_loss: 0.9677 - val_acc: 0.5378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x51550240>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_twit = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=(100,),activation=tf.nn.tanh),\n",
    "    keras.layers.Dense(80, activation=tf.nn.relu),\n",
    "#     keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "])\n",
    "model_twit.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_twit.fit(t,twit_label,epochs=10,validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
