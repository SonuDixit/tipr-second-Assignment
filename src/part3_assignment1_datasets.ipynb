{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helper_fns_from_1 import data_read_convert_to_np_array, preprocess_text_data, read_label_from_text_file\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from NN import NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dolphin dataset, MLNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolph_data = data_read_convert_to_np_array(data_path = \"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\dolphins\\\\dolphins.csv\")\n",
    "dolph_label = data_read_convert_to_np_array(data_path = \"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\dolphins\\\\dolphins_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dolph_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(dolph_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dolph_data, dolph_label, test_size=0.20, random_state=42,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_one_hot = np.zeros((y_train.shape[0],4))\n",
    "y_test_one_hot = np.zeros((y_test.shape[0],4))\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_tr_one_hot[i,y_train[i]] = 1\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_one_hot[i,y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.Sequential([\n",
    "    keras.layers.Dense(30, input_shape=(32,),activation=tf.nn.tanh),\n",
    "    keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(4, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49 samples, validate on 13 samples\n",
      "Epoch 1/19\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 1.5054 - acc: 0.0408 - val_loss: 1.4818 - val_acc: 0.0769\n",
      "Epoch 2/19\n",
      "49/49 [==============================] - 0s 861us/step - loss: 1.4663 - acc: 0.1837 - val_loss: 1.4449 - val_acc: 0.3077\n",
      "Epoch 3/19\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4291 - acc: 0.2653 - val_loss: 1.4139 - val_acc: 0.3077\n",
      "Epoch 4/19\n",
      "49/49 [==============================] - 0s 694us/step - loss: 1.4013 - acc: 0.3265 - val_loss: 1.3857 - val_acc: 0.3077\n",
      "Epoch 5/19\n",
      "49/49 [==============================] - 0s 551us/step - loss: 1.3721 - acc: 0.2857 - val_loss: 1.3594 - val_acc: 0.3077\n",
      "Epoch 6/19\n",
      "49/49 [==============================] - 0s 388us/step - loss: 1.3500 - acc: 0.3061 - val_loss: 1.3350 - val_acc: 0.2308\n",
      "Epoch 7/19\n",
      "49/49 [==============================] - 0s 265us/step - loss: 1.3275 - acc: 0.2857 - val_loss: 1.3201 - val_acc: 0.2308\n",
      "Epoch 8/19\n",
      "49/49 [==============================] - 0s 245us/step - loss: 1.3140 - acc: 0.2857 - val_loss: 1.3124 - val_acc: 0.2308\n",
      "Epoch 9/19\n",
      "49/49 [==============================] - 0s 592us/step - loss: 1.3037 - acc: 0.2857 - val_loss: 1.3051 - val_acc: 0.2308\n",
      "Epoch 10/19\n",
      "49/49 [==============================] - 0s 367us/step - loss: 1.2923 - acc: 0.2857 - val_loss: 1.2977 - val_acc: 0.2308\n",
      "Epoch 11/19\n",
      "49/49 [==============================] - 0s 347us/step - loss: 1.2801 - acc: 0.2653 - val_loss: 1.2902 - val_acc: 0.2308\n",
      "Epoch 12/19\n",
      "49/49 [==============================] - 0s 674us/step - loss: 1.2678 - acc: 0.2653 - val_loss: 1.2831 - val_acc: 0.2308\n",
      "Epoch 13/19\n",
      "49/49 [==============================] - 0s 306us/step - loss: 1.2579 - acc: 0.2653 - val_loss: 1.2756 - val_acc: 0.2308\n",
      "Epoch 14/19\n",
      "49/49 [==============================] - 0s 204us/step - loss: 1.2459 - acc: 0.2653 - val_loss: 1.2679 - val_acc: 0.2308\n",
      "Epoch 15/19\n",
      "49/49 [==============================] - 0s 674us/step - loss: 1.2358 - acc: 0.2449 - val_loss: 1.2599 - val_acc: 0.2308\n",
      "Epoch 16/19\n",
      "49/49 [==============================] - 0s 306us/step - loss: 1.2251 - acc: 0.2449 - val_loss: 1.2527 - val_acc: 0.2308\n",
      "Epoch 17/19\n",
      "49/49 [==============================] - 0s 245us/step - loss: 1.2137 - acc: 0.2449 - val_loss: 1.2457 - val_acc: 0.2308\n",
      "Epoch 18/19\n",
      "49/49 [==============================] - 0s 735us/step - loss: 1.2043 - acc: 0.2449 - val_loss: 1.2391 - val_acc: 0.2308\n",
      "Epoch 19/19\n",
      "49/49 [==============================] - 0s 347us/step - loss: 1.1944 - acc: 0.2449 - val_loss: 1.2331 - val_acc: 0.2308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3746de10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(dolph_data,dolph_label,epochs=19,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dolph = NN(hidden_layer=[30,4], \n",
    "                activation=[\"tanh\",\"relu\"], \n",
    "                input_dim=32,output_dim=4,momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 loss =1.2733400474010232\n",
      "epoch = 2 loss =0.9834740615602726\n",
      "epoch = 3 loss =0.7764339577684372\n",
      "epoch = 4 loss =0.7190874529430265\n",
      "epoch = 5 loss =0.6828130177420096\n",
      "epoch = 6 loss =0.6550770256681598\n",
      "epoch = 7 loss =0.6139541926149509\n",
      "epoch = 8 loss =0.5387398214174133\n",
      "epoch = 9 loss =0.4247510288675874\n",
      "epoch = 10 loss =0.3559162716691545\n",
      "epoch = 11 loss =0.34251144907647585\n",
      "epoch = 12 loss =0.30578019245826377\n",
      "epoch = 13 loss =0.30459097644509386\n",
      "epoch = 14 loss =0.268126036812998\n",
      "epoch = 15 loss =0.319825866994034\n",
      "epoch = 16 loss =0.28670025812064764\n",
      "epoch = 17 loss =0.23010552869704096\n",
      "epoch = 18 loss =0.24635910048538306\n",
      "epoch = 19 loss =0.17590524670771437\n",
      "epoch = 20 loss =0.1928258258098528\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "loss = net_dolph.fit_batch(X_train,y_tr_one_hot,epochs=20,lr=0.1,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals = net_dolph.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,pred_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is the best for dolphin dataset, it is worse than Nearest neighbor classifier. with the library function the accuracy is poorer than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"net_dolph.pickle\",\"wb\") as f:\n",
    "    pickle.dump(net_dolph,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pubmed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_data = data_read_convert_to_np_array(data_path = \"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\pubmed\\\\pubmed.csv\")\n",
    "pubmed_label = data_read_convert_to_np_array(data_path = \"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\pubmed\\\\pubmed_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20701, 128)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(pubmed_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pubmed_data, pubmed_label, test_size=0.40, random_state=42,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_one_hot = np.zeros((y_train.shape[0],3))\n",
    "y_test_one_hot = np.zeros((y_test.shape[0],3))\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_tr_one_hot[i,y_train[i]] = 1\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_one_hot[i,y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pubmed = NN(hidden_layer=[30,30,10], \n",
    "                activation=[\"tanh\",\"relu\",\"relu\"], \n",
    "                input_dim=128,output_dim=3,\n",
    "                momentum=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12420, 128)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 loss =1.0663808730471807\n",
      "epoch = 2 loss =1.0643598324872867\n",
      "epoch = 3 loss =1.0642069286906986\n",
      "epoch = 4 loss =1.0638394096317438\n",
      "epoch = 5 loss =1.0627155950240816\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "loss = net_pubmed.fit_batch(X_train,y_tr_one_hot,epochs=5,lr=0.001,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 loss =1.064075247200067\n",
      "epoch = 2 loss =1.0621916471515933\n",
      "epoch = 3 loss =1.0613814311825533\n",
      "epoch = 4 loss =1.0608073996330214\n",
      "epoch = 5 loss =1.059810743603916\n",
      "epoch = 6 loss =1.0590234898180506\n",
      "epoch = 7 loss =1.0584063486588082\n",
      "epoch = 8 loss =1.057750572495492\n",
      "epoch = 9 loss =1.0574023024191417\n",
      "epoch = 10 loss =1.0565003738417122\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "loss = net_pubmed.fit_batch(X_train,y_tr_one_hot,epochs=10,lr=0.01,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4366622388600411"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vals = net_pubmed.predict(X_test)\n",
    "accuracy_score(y_test,pred_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"net_pubmed.pickle\",\"wb\") as f:\n",
    "    pickle.dump(net_pubmed,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pubmed2 = NN(hidden_layer=[30,10], \n",
    "                activation=[\"tanh\",\"relu\"], \n",
    "                input_dim=128,\n",
    "                output_dim=3,\n",
    "                momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 loss =1.0637150393222712\n",
      "epoch = 2 loss =1.058027019830325\n",
      "epoch = 3 loss =1.0535598759935445\n",
      "epoch = 4 loss =1.0536369607481602\n",
      "epoch = 5 loss =1.0497450017517609\n",
      "epoch = 6 loss =1.0480842051762636\n",
      "epoch = 7 loss =1.0464809521385992\n",
      "epoch = 8 loss =1.0472049893016864\n",
      "epoch = 9 loss =1.0473097660752162\n",
      "epoch = 10 loss =1.0415121774149085\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "loss = net_pubmed2.fit_batch(X_train,y_tr_one_hot,epochs=10,lr=0.01,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3981403212172443"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vals = net_pubmed2.predict(X_test)\n",
    "accuracy_score(y_test,pred_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12420 samples, validate on 8281 samples\n",
      "Epoch 1/10\n",
      "12420/12420 [==============================] - 3s 248us/step - loss: 1.0679 - acc: 0.4141 - val_loss: 1.0645 - val_acc: 0.4187\n",
      "Epoch 2/10\n",
      "12420/12420 [==============================] - 3s 223us/step - loss: 1.0524 - acc: 0.4405 - val_loss: 1.0631 - val_acc: 0.4263\n",
      "Epoch 3/10\n",
      "12420/12420 [==============================] - 3s 216us/step - loss: 1.0461 - acc: 0.4500 - val_loss: 1.0690 - val_acc: 0.4282\n",
      "Epoch 4/10\n",
      "12420/12420 [==============================] - 3s 220us/step - loss: 1.0427 - acc: 0.4541 - val_loss: 1.0659 - val_acc: 0.4276\n",
      "Epoch 5/10\n",
      "12420/12420 [==============================] - 3s 212us/step - loss: 1.0389 - acc: 0.4573 - val_loss: 1.0708 - val_acc: 0.4236\n",
      "Epoch 6/10\n",
      "12420/12420 [==============================] - 3s 212us/step - loss: 1.0360 - acc: 0.4592 - val_loss: 1.0737 - val_acc: 0.4256\n",
      "Epoch 7/10\n",
      "12420/12420 [==============================] - 3s 217us/step - loss: 1.0327 - acc: 0.4648 - val_loss: 1.0732 - val_acc: 0.4231\n",
      "Epoch 8/10\n",
      "12420/12420 [==============================] - 3s 220us/step - loss: 1.0293 - acc: 0.4645 - val_loss: 1.0788 - val_acc: 0.4269\n",
      "Epoch 9/10\n",
      "12420/12420 [==============================] - 3s 205us/step - loss: 1.0260 - acc: 0.4742 - val_loss: 1.0818 - val_acc: 0.4224\n",
      "Epoch 10/10\n",
      "12420/12420 [==============================] - 2s 137us/step - loss: 1.0228 - acc: 0.4719 - val_loss: 1.0803 - val_acc: 0.4239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3bed2470>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = keras.Sequential([\n",
    "    keras.layers.Dense(40, input_shape=(128,),activation=tf.nn.tanh),\n",
    "    keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "#     keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "])\n",
    "model2.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model2.fit(pubmed_data,pubmed_label,epochs=10,validation_split = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12420 samples, validate on 8281 samples\n",
      "Epoch 1/10\n",
      "12420/12420 [==============================] - 2s 146us/step - loss: 1.0077 - acc: 0.4891 - val_loss: 1.0967 - val_acc: 0.4107\n",
      "Epoch 2/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 1.0035 - acc: 0.4977 - val_loss: 1.0956 - val_acc: 0.4090\n",
      "Epoch 3/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 0.9993 - acc: 0.4980 - val_loss: 1.1036 - val_acc: 0.4056\n",
      "Epoch 4/10\n",
      "12420/12420 [==============================] - 2s 148us/step - loss: 0.9947 - acc: 0.5035 - val_loss: 1.1039 - val_acc: 0.4006\n",
      "Epoch 5/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 0.9896 - acc: 0.5054 - val_loss: 1.1139 - val_acc: 0.4014\n",
      "Epoch 6/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 0.9875 - acc: 0.5137 - val_loss: 1.1158 - val_acc: 0.4031\n",
      "Epoch 7/10\n",
      "12420/12420 [==============================] - 2s 147us/step - loss: 0.9821 - acc: 0.5129 - val_loss: 1.1220 - val_acc: 0.3998\n",
      "Epoch 8/10\n",
      "12420/12420 [==============================] - 2s 145us/step - loss: 0.9778 - acc: 0.5217 - val_loss: 1.1211 - val_acc: 0.3990\n",
      "Epoch 9/10\n",
      "12420/12420 [==============================] - 2s 146us/step - loss: 0.9738 - acc: 0.5201 - val_loss: 1.1381 - val_acc: 0.3905\n",
      "Epoch 10/10\n",
      "12420/12420 [==============================] - 2s 146us/step - loss: 0.9707 - acc: 0.5221 - val_loss: 1.1390 - val_acc: 0.3951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3ff1c4e0>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(pubmed_data,pubmed_label,epochs=10,validation_split = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_data = preprocess_text_data(\"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\twitter\\\\twitter.txt\")\n",
    "twit_label = read_label_from_text_file(\"H:\\\\sonu\\\\tipr\\\\Assignment2\\\\Data\\\\twitter\\\\twitter_label.txt\")\n",
    "twit_label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(twit_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 2845)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/10\n",
      "4200/4200 [==============================] - 1s 256us/step - loss: 0.9604 - acc: 0.4893 - val_loss: 0.9293 - val_acc: 0.5456\n",
      "Epoch 2/10\n",
      "4200/4200 [==============================] - 1s 140us/step - loss: 0.8642 - acc: 0.5852 - val_loss: 0.9237 - val_acc: 0.5472\n",
      "Epoch 3/10\n",
      "4200/4200 [==============================] - 1s 140us/step - loss: 0.8356 - acc: 0.6017 - val_loss: 0.9321 - val_acc: 0.5561\n",
      "Epoch 4/10\n",
      "4200/4200 [==============================] - 1s 142us/step - loss: 0.8153 - acc: 0.6212 - val_loss: 0.9349 - val_acc: 0.5589\n",
      "Epoch 5/10\n",
      "4200/4200 [==============================] - 1s 142us/step - loss: 0.7928 - acc: 0.6283 - val_loss: 0.9388 - val_acc: 0.5578\n",
      "Epoch 6/10\n",
      "4200/4200 [==============================] - 1s 147us/step - loss: 0.7733 - acc: 0.6395 - val_loss: 0.9518 - val_acc: 0.5572\n",
      "Epoch 7/10\n",
      "4200/4200 [==============================] - 1s 139us/step - loss: 0.7459 - acc: 0.6595 - val_loss: 0.9771 - val_acc: 0.5494\n",
      "Epoch 8/10\n",
      "4200/4200 [==============================] - 1s 139us/step - loss: 0.7177 - acc: 0.6750 - val_loss: 0.9969 - val_acc: 0.5311\n",
      "Epoch 9/10\n",
      "4200/4200 [==============================] - 1s 145us/step - loss: 0.6895 - acc: 0.6976 - val_loss: 1.0117 - val_acc: 0.5467\n",
      "Epoch 10/10\n",
      "4200/4200 [==============================] - 1s 137us/step - loss: 0.6601 - acc: 0.7029 - val_loss: 1.0488 - val_acc: 0.5333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3746d1d0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_twit = keras.Sequential([\n",
    "    keras.layers.Dense(300, input_shape=(100,),activation=tf.nn.tanh),\n",
    "    keras.layers.Dense(80, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(30, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "])\n",
    "model_twit.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_twit.fit(t,twit_label,epochs=10,validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_twitter = NN(hidden_layer=[300,30,30], \n",
    "                activation=[\"tanh\",\"relu\",\"relu\"], \n",
    "                input_dim=2845,\n",
    "                output_dim=3,\n",
    "                momentum=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 2845)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    twit_data, twit_label, test_size=0.40, random_state=42,shuffle = True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_one_hot = np.zeros((y_train.shape[0],3))\n",
    "y_test_one_hot = np.zeros((y_test.shape[0],3))\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_tr_one_hot[i,y_train[i]] = 1\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_one_hot[i,y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 loss =1.0125442205555961\n",
      "epoch = 2 loss =0.9866258045853535\n",
      "epoch = 3 loss =0.986749538669689\n",
      "epoch = 4 loss =0.9867547809889617\n",
      "epoch = 5 loss =0.9867500110958901\n",
      "epoch = 6 loss =0.9867443085375163\n",
      "epoch = 7 loss =0.9867379875269499\n",
      "epoch = 8 loss =0.9867304799715169\n",
      "epoch = 9 loss =0.9867223796924184\n",
      "epoch = 10 loss =0.9867133336642115\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "loss = net_twitter.fit_batch(X_train,y_tr_one_hot,epochs=10,lr=0.001,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 loss =0.9904289481503014\n",
      "epoch = 2 loss =0.990483923904595\n",
      "epoch = 3 loss =0.9898723559383319\n",
      "epoch = 4 loss =0.9815211383427285\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "loss = net_twitter.fit_batch(X_train,y_tr_one_hot,epochs=4,lr=0.01,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5183333333333333"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vals = net_twitter.predict(X_test)\n",
    "accuracy_score(y_test,pred_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"net_twitter.pickle\",\"wb\") as f:\n",
    "    pickle.dump(net_twitter,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
